{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "another-acrobat",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Denylson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Denylson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Denylson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#Using for training and testing data creation\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import keras as keras\n",
    "\n",
    "#Used for model creation \n",
    "import torch as torch\n",
    "from torch import nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fourth-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name = '42B', dim = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "informed-conversion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2381\n"
     ]
    }
   ],
   "source": [
    "news1 = pd.read_csv('./djia_news/djia_news copy.csv')\n",
    "news2 = pd.read_csv('./nasdaq/nasdaq.csv')\n",
    "\n",
    "# combined_news = news1.append(news2)\n",
    "print(len(news1))\n",
    "news2 = news2.replace([0,1,2], ['decrease', 'increase', 'constant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "built-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Get_Modify():\n",
    "    \n",
    "    def __init__(self, dataframe, max_length):\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "        self.dataframe = self.dataframe[['Label', 'Headline']]\n",
    "        \n",
    "        self.label_dict = {'decrease' : 0, 'increase' : 1, 'constant' : 2}\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        self.dataframe = self.dataframe.apply(lambda x: x.astype(str).str.lower())\n",
    "        self.dataframe.Headline = self.dataframe.Headline.str.replace('[@:]','')\n",
    "        self.dataframe.Headline = self.dataframe.Headline.str.replace('totestravel', '')\n",
    "        self.dataframe['Headline_tokens'] = self.dataframe['Headline'].apply(lambda x: word_tokenize(x))\n",
    "        \n",
    "        all_words = []\n",
    "        for entry in self.dataframe['Headline_tokens']:\n",
    "            all_words += entry\n",
    "        frequency = Counter(all_words)\n",
    "        \n",
    "        self.vocab = torchtext.vocab.Vocab(counter = frequency, min_freq = 5, vectors = glove)\n",
    "        \n",
    "        self.dataframe = pd.DataFrame(self.dataframe)\n",
    "\n",
    "        #print(self.dataframe.head())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe['Headline'])\n",
    "    \n",
    "    def back_to_text(self, tokens):\n",
    "        text = ''\n",
    "        for token in tokens:\n",
    "            text += self.vocab.itos[token] + \" \"\n",
    "        return text\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label = self.label_dict[self.dataframe['Label'][index]]\n",
    "        label = torch.tensor(label)\n",
    "        \n",
    "        int_tokens = []\n",
    "        headline_tokens = self.dataframe['Headline_tokens'][index]\n",
    "        for token in headline_tokens:\n",
    "            int_tokens.append(self.vocab[token])\n",
    "        \n",
    "        if len(int_tokens) < self.max_length:\n",
    "            num_to_pad = self.max_length - len(int_tokens)\n",
    "            int_tokens += [0] * num_to_pad\n",
    "        else:\n",
    "            int_tokens = int_tokens[:self.max_length]\n",
    "        int_tokens = torch.tensor(int_tokens)\n",
    "        return(int_tokens, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "distributed-degree",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denylson\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6343"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Retreving data from csv files and merge both datasets\n",
    "news = Get_Modify(news2, 75)\n",
    "vocab = news.vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "laden-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determining training and test indices \n",
    "\n",
    "train_amount = int(0.70 * len(news))\n",
    "train_indices = list(range(0, train_amount))\n",
    "test_indices = list(range(train_amount, len(news)))\n",
    "\n",
    "#Creating subsets using the indices determined above\n",
    "training_data = Subset(news, train_indices)\n",
    "testing_data = Subset(news, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "amended-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the data to feed into model \n",
    "\n",
    "training_generator = DataLoader(training_data, batch_size = 8, shuffle = True)\n",
    "testing_generator = DataLoader(testing_data, batch_size = 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "stuck-raising",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 75]), torch.Size([8]))"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_data, batched_labels = next(iter(training_generator))\n",
    "# output = news.dataframe['Label'][1]\n",
    "# news.label_dict[news.dataframe['Label'][1]]\n",
    "batched_data.shape, batched_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "south-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creationg of the NLP model\n",
    "class News_NLP(nn.Module):\n",
    "    def __init__(self, num_words, emb_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_words = num_words\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        self.emb = nn.Embedding(self.num_words, self.emb_size)\n",
    "        self.emb.from_pretrained(vocab.vectors)\n",
    "        self.gru = nn.GRU(input_size = emb_size, hidden_size = 32, batch_first = True, num_layers = 2)\n",
    "        self.lstm = nn.LSTM(input_size = 32, hidden_size = 32, batch_first= True, num_layers = 2, dropout = 0.1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, batch_data):\n",
    "        token_embs = self.emb(batch_data)\n",
    "        outputs, (h_n, c_n) = self.gru(token_embs)\n",
    "        outputs, (h_n, c_n) = self.lstm(outputs)\n",
    "        \n",
    "        last_hidden_state = h_n\n",
    "        last_hidden_state = last_hidden_state.permute(1, 0, 2)\n",
    "        last_hidden_state = last_hidden_state.flatten(start_dim = 1)\n",
    "        last_hidden_state = self.sig(last_hidden_state)\n",
    "        \n",
    "        logits = self.linear(last_hidden_state)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "august-margin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News_NLP(\n",
       "  (emb): Embedding(6343, 300)\n",
       "  (gru): GRU(300, 32, num_layers=2, batch_first=True)\n",
       "  (lstm): LSTM(32, 32, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (sig): Sigmoid()\n",
       "  (linear): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = News_NLP(num_words = len(vocab), emb_size = 300, num_classes = 3)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "atlantic-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "imported-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "toxic-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(preds, batched_labels):\n",
    "    predicted_labels = torch.softmax(preds, dim = 1).argmax(dim = 1)\n",
    "    num_correct = (predicted_labels == batched_labels).sum()\n",
    "    \n",
    "    acc = num_correct/len(batched_labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "charged-netscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Train: loss : 1.1169962882995605, accuracy: 0.125\n",
      "Train: loss : 1.0466067790985107, accuracy: 0.5\n",
      "Train: loss : 0.5740376710891724, accuracy: 0.875\n",
      "Train: loss : 0.5023205280303955, accuracy: 0.875\n",
      "Train: loss : 0.8633860945701599, accuracy: 0.75\n",
      "Train: loss : 1.1283587217330933, accuracy: 0.375\n",
      "Train: loss : 0.7542713284492493, accuracy: 0.5\n",
      "Train: loss : 0.6130088567733765, accuracy: 0.75\n",
      "Train: loss : 0.7151778936386108, accuracy: 0.5\n",
      "Train: loss : 0.7484060525894165, accuracy: 0.375\n",
      "Train: loss : 0.5744444131851196, accuracy: 0.75\n",
      "Train: loss : 0.8306998014450073, accuracy: 0.375\n",
      "Train: loss : 0.6781908869743347, accuracy: 0.625\n",
      "Train: loss : 0.5825556516647339, accuracy: 0.75\n",
      "Train: loss : 1.1113386154174805, accuracy: 0.5\n",
      "Train: loss : 0.7053288817405701, accuracy: 0.625\n",
      "Train: loss : 0.6712198257446289, accuracy: 0.75\n",
      "Train: loss : 0.887102484703064, accuracy: 0.125\n",
      "Train: loss : 0.6278265714645386, accuracy: 0.75\n",
      "Train: loss : 0.3422941565513611, accuracy: 1.0\n",
      "Train: loss : 0.6485074758529663, accuracy: 0.75\n",
      "Train: loss : 1.2267565727233887, accuracy: 0.375\n",
      "Train: loss : 0.9234897494316101, accuracy: 0.5\n",
      "Train: loss : 0.6236207485198975, accuracy: 0.75\n",
      "Test: loss: 2.4256696701049805, accuracy: 0.375\n",
      "Test: loss: 0.8682650923728943, accuracy: 0.625\n",
      "Test: loss: 0.6618379354476929, accuracy: 0.875\n",
      "Test: loss: 0.15333057940006256, accuracy: 1.0\n",
      "Test: loss: 0.9001491665840149, accuracy: 0.75\n",
      "Test: loss: 0.15333126485347748, accuracy: 1.0\n",
      "Test: loss: 1.376772165298462, accuracy: 0.5\n",
      "Test: loss: 0.15332640707492828, accuracy: 1.0\n",
      "Test: loss: 2.123579978942871, accuracy: 0.25\n",
      "Test: loss: 0.39164286851882935, accuracy: 0.875\n",
      "------------------------------------------------------------\n",
      "Train: loss : 0.8682685494422913, accuracy: 0.625\n",
      "Train: loss : 0.7167343497276306, accuracy: 0.5\n",
      "Train: loss : 1.2364306449890137, accuracy: 0.375\n",
      "Train: loss : 0.7840141654014587, accuracy: 0.25\n",
      "Train: loss : 0.4798731803894043, accuracy: 0.875\n",
      "Train: loss : 0.7206485271453857, accuracy: 0.625\n",
      "Train: loss : 1.2457112073898315, accuracy: 0.375\n",
      "Train: loss : 0.9872606992721558, accuracy: 0.75\n",
      "Train: loss : 0.9006553888320923, accuracy: 0.75\n",
      "Train: loss : 0.9919981956481934, accuracy: 0.75\n",
      "Train: loss : 0.7053420543670654, accuracy: 0.75\n",
      "Train: loss : 1.1495230197906494, accuracy: 0.5\n",
      "Train: loss : 1.144330620765686, accuracy: 0.625\n",
      "Train: loss : 0.9430354833602905, accuracy: 0.75\n",
      "Train: loss : 1.0726611614227295, accuracy: 0.625\n",
      "Train: loss : 0.9471716284751892, accuracy: 0.375\n",
      "Train: loss : 0.49589550495147705, accuracy: 0.875\n",
      "Train: loss : 1.2882585525512695, accuracy: 0.625\n",
      "Train: loss : 0.2414238601922989, accuracy: 1.0\n",
      "Train: loss : 1.665373682975769, accuracy: 0.625\n",
      "Train: loss : 0.9819586277008057, accuracy: 0.625\n",
      "Train: loss : 1.1791859865188599, accuracy: 0.375\n",
      "Train: loss : 0.8872647881507874, accuracy: 0.5\n",
      "Train: loss : 0.4804500341415405, accuracy: 0.875\n",
      "Test: loss: 3.222346067428589, accuracy: 0.375\n",
      "Test: loss: 0.6827329397201538, accuracy: 0.625\n",
      "Test: loss: 1.0357779264450073, accuracy: 0.875\n",
      "Test: loss: 0.3415728807449341, accuracy: 1.0\n",
      "Test: loss: 1.1419025659561157, accuracy: 0.75\n",
      "Test: loss: 0.3457965552806854, accuracy: 1.0\n",
      "Test: loss: 1.369200587272644, accuracy: 0.5\n",
      "Test: loss: 0.3426437973976135, accuracy: 1.0\n",
      "Test: loss: 2.1749823093414307, accuracy: 0.25\n",
      "Test: loss: 0.4600176215171814, accuracy: 0.875\n",
      "------------------------------------------------------------\n",
      "Train: loss : 0.9013303518295288, accuracy: 0.375\n",
      "Train: loss : 0.820967972278595, accuracy: 0.375\n",
      "Train: loss : 0.604213297367096, accuracy: 0.875\n",
      "Train: loss : 0.5967870354652405, accuracy: 0.75\n",
      "Train: loss : 1.1255269050598145, accuracy: 0.25\n",
      "Train: loss : 0.6876199245452881, accuracy: 0.75\n",
      "Train: loss : 0.7007923126220703, accuracy: 0.625\n",
      "Train: loss : 1.253419041633606, accuracy: 0.5\n",
      "Train: loss : 0.9985278248786926, accuracy: 0.375\n",
      "Train: loss : 0.7206096649169922, accuracy: 0.5\n",
      "Train: loss : 0.9178512096405029, accuracy: 0.375\n",
      "Train: loss : 0.7971384525299072, accuracy: 0.5\n",
      "Train: loss : 0.26039212942123413, accuracy: 1.0\n",
      "Train: loss : 0.7486648559570312, accuracy: 0.625\n",
      "Train: loss : 0.7704325914382935, accuracy: 0.25\n",
      "Train: loss : 1.368897795677185, accuracy: 0.25\n",
      "Train: loss : 0.6902981996536255, accuracy: 1.0\n",
      "Train: loss : 0.6881879568099976, accuracy: 0.625\n",
      "Train: loss : 0.6058764457702637, accuracy: 0.75\n",
      "Train: loss : 1.0197845697402954, accuracy: 0.75\n",
      "Train: loss : 1.1812156438827515, accuracy: 0.375\n",
      "Train: loss : 0.5809988379478455, accuracy: 0.75\n",
      "Train: loss : 1.0414628982543945, accuracy: 0.375\n",
      "Train: loss : 0.8872261643409729, accuracy: 0.25\n",
      "Test: loss: 2.239668607711792, accuracy: 0.375\n",
      "Test: loss: 0.7119436860084534, accuracy: 0.625\n",
      "Test: loss: 0.7723428010940552, accuracy: 0.875\n",
      "Test: loss: 0.3261692523956299, accuracy: 1.0\n",
      "Test: loss: 0.9009574055671692, accuracy: 0.75\n",
      "Test: loss: 0.3261064291000366, accuracy: 1.0\n",
      "Test: loss: 1.158200979232788, accuracy: 0.5\n",
      "Test: loss: 0.3261042833328247, accuracy: 1.0\n",
      "Test: loss: 1.733352541923523, accuracy: 0.25\n",
      "Test: loss: 0.4547242522239685, accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"-\" * 60)\n",
    "    for index, (batched_data, batched_labels) in enumerate(training_generator):\n",
    "        preds = model(batched_data)\n",
    "        loss = loss_func(preds, batched_labels)\n",
    "        accuracy = cal_acc(preds, batched_labels)\n",
    "        if( index % 50 == 0):\n",
    "            print(\"Train: loss : {0}, accuracy: {1}\".format(loss, accuracy))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    for index, (batched_data, batched_labels) in enumerate(testing_generator):\n",
    "        preds = model(batched_data)\n",
    "        loss = loss_func(preds, batched_labels)\n",
    "        accuracy = cal_acc(preds, batched_labels)\n",
    "        if(index % 50 == 0):\n",
    "            print(\"Test: loss: {0}, accuracy: {1}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-survivor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
