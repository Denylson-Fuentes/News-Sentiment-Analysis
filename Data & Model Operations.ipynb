{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "another-acrobat",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Denylson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Denylson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Denylson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#Using for training and testing data creation\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import keras as keras\n",
    "\n",
    "#Used for model creation \n",
    "import torch as torch\n",
    "from torch import nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fourth-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name = '6B', dim = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "informed-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "news1 = pd.read_csv('./data/djia_news copy.csv')\n",
    "news2 = pd.read_csv('./data/nasdaq.csv')\n",
    "# frames = [news1, news2]\n",
    "\n",
    "# combined_news = pd.concat(frames)\n",
    "# combined_news.drop(['Ticker'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b29b293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1430\n",
      "1     936\n",
      "2      15\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# class_count_0, class_count_1, class_count_2 = combined_news['Label'].value_counts()\n",
    "\n",
    "news3 = pd.read_csv('./data/train.csv')\n",
    "# news3 = news3[news3['Label'] == 'neutral']\n",
    "# news3 = news3.sample(class_count_1)\n",
    "print(news1['Label'].value_counts())\n",
    "\n",
    "# frames  = [combined_news, news3]\n",
    "# combined_news = pd.concat(frames)\n",
    "combined_news = news1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49673956",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_news['Headline'] = combined_news['Headline'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "built-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Get_Modify():\n",
    "    \n",
    "    def __init__(self, dataframe, max_length):\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "        self.dataframe = self.dataframe[['Label', 'Headline']]\n",
    "        self.label_dict = {'0' : 0, '1' : 1, '2' : 2}\n",
    "\n",
    "        \n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        self.dataframe = self.dataframe.apply(lambda x: x.astype(str).str.lower())\n",
    "        self.dataframe.Headline = self.dataframe.Headline.str.replace('[@:]','')\n",
    "        self.dataframe.Headline = self.dataframe.Headline.str.replace('totestravel', '')\n",
    "    \n",
    "        self.dataframe['Headline_tokens'] = self.dataframe['Headline'].apply(lambda x: word_tokenize(x))\n",
    "        \n",
    "        all_words = []\n",
    "        for entry in self.dataframe['Headline_tokens']:\n",
    "            all_words += entry\n",
    "        frequency = Counter(all_words)\n",
    "        \n",
    "        self.vocab = torchtext.vocab.Vocab(counter = frequency, min_freq = 5, vectors = glove)\n",
    "        \n",
    "        self.dataframe = pd.DataFrame(self.dataframe)\n",
    "\n",
    "        #print(self.dataframe.head())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe['Headline'])\n",
    "    \n",
    "    def back_to_text(self, tokens):\n",
    "        text = ''\n",
    "        for token in tokens:\n",
    "            text += self.vocab.itos[token] + \" \"\n",
    "        return text\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label = self.label_dict[self.dataframe['Label'][index]]\n",
    "        label = torch.tensor(label)\n",
    "        \n",
    "        int_tokens = []\n",
    "        headline_tokens = self.dataframe['Headline_tokens'][index]\n",
    "        for token in headline_tokens:\n",
    "            int_tokens.append(self.vocab[token])\n",
    "        \n",
    "        if len(int_tokens) < self.max_length:\n",
    "            num_to_pad = self.max_length - len(int_tokens)\n",
    "            int_tokens += [0] * num_to_pad\n",
    "        else:\n",
    "            int_tokens = int_tokens[:self.max_length]\n",
    "        int_tokens = torch.tensor(int_tokens)\n",
    "        return(int_tokens, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "distributed-degree",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1477"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Retreving data from csv files and merge both datasets\n",
    "news = Get_Modify(combined_news, 75)\n",
    "vocab = news.vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "laden-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determining training and test indices \n",
    "\n",
    "train_amount = int(0.70 * len(news))\n",
    "train_indices = list(range(0, train_amount))\n",
    "test_indices = list(range(train_amount, len(news)))\n",
    "\n",
    "#Creating subsets using the indices determined above\n",
    "training_data = Subset(news, train_indices)\n",
    "testing_data = Subset(news, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "amended-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the data to feed into model \n",
    "\n",
    "training_generator = DataLoader(training_data, batch_size = 8, shuffle = True)\n",
    "testing_generator = DataLoader(testing_data, batch_size = 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "stuck-raising",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 75]), torch.Size([8]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_data, batched_labels = next(iter(training_generator))\n",
    "batched_data.shape, batched_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "south-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creationg of the NLP model\n",
    "class News_NLP(nn.Module):\n",
    "    def __init__(self, num_words, emb_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_words = num_words\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        self.emb = nn.Embedding(self.num_words, self.emb_size)\n",
    "        self.emb.from_pretrained(vocab.vectors)\n",
    "        self.lstm = nn.LSTM(input_size = emb_size, hidden_size = 16, batch_first = True, num_layers = 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(32, num_classes)\n",
    "    \n",
    "    def forward(self, batch_data):\n",
    "        token_embs = self.emb(batch_data)\n",
    "        outputs, (h_n, c_n) = self.lstm(token_embs)\n",
    "        \n",
    "\n",
    "        \n",
    "        last_hidden_state = h_n\n",
    "        last_hidden_state = last_hidden_state.permute(1, 0, 2)\n",
    "        last_hidden_state = last_hidden_state.flatten(start_dim = 1)\n",
    "        \n",
    "        last_hidden_state = self.relu(last_hidden_state)\n",
    "        logits = self.linear(last_hidden_state)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "august-margin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News_NLP(\n",
       "  (emb): Embedding(1477, 300)\n",
       "  (lstm): LSTM(300, 16, num_layers=2, batch_first=True)\n",
       "  (relu): ReLU()\n",
       "  (linear): Linear(in_features=32, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = News_NLP(num_words = len(vocab), emb_size = 300, num_classes = 3)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "atlantic-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "imported-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "toxic-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(preds, batched_labels):\n",
    "    predicted_labels = torch.softmax(preds, dim = 1).argmax(dim = 1)\n",
    "    count = 0\n",
    "    for label in range(0, len(predicted_labels)):\n",
    "            if( batched_labels[label] ==  predicted_labels[label]):\n",
    "                count = count + 1\n",
    "        \n",
    "    acc = count/len(batched_labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "charged-netscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Train: loss : 0.845587432384491, accuracy: 0.5\n",
      "Train: loss : 0.6741369962692261, accuracy: 0.625\n",
      "Train: loss : 0.9073151350021362, accuracy: 0.375\n",
      "Train: loss : 0.6957859992980957, accuracy: 0.5\n",
      "Train: loss : 0.5660184621810913, accuracy: 0.75\n",
      "Test: loss: 0.6952836513519287, accuracy: 0.5\n",
      "Test: loss: 0.7380596995353699, accuracy: 0.125\n",
      "Test: loss: 1.5629606246948242, accuracy: 0.375\n",
      "------------------------------------------------------------\n",
      "Train: loss : 0.652507483959198, accuracy: 0.875\n",
      "Train: loss : 0.7088024616241455, accuracy: 0.5\n",
      "Train: loss : 0.6642118692398071, accuracy: 0.625\n",
      "Train: loss : 0.5458628535270691, accuracy: 0.875\n",
      "Train: loss : 1.1399377584457397, accuracy: 0.375\n",
      "Test: loss: 0.7182468771934509, accuracy: 0.5\n",
      "Test: loss: 0.5602640509605408, accuracy: 0.875\n",
      "Test: loss: 1.3253257274627686, accuracy: 0.5\n",
      "------------------------------------------------------------\n",
      "Train: loss : 0.8235688209533691, accuracy: 0.25\n",
      "Train: loss : 0.5894268155097961, accuracy: 0.75\n",
      "Train: loss : 0.7097429633140564, accuracy: 0.5\n",
      "Train: loss : 0.6798988580703735, accuracy: 1.0\n",
      "Train: loss : 0.702075719833374, accuracy: 0.5\n",
      "Test: loss: 0.8520470857620239, accuracy: 0.5\n",
      "Test: loss: 1.2858060598373413, accuracy: 0.125\n",
      "Test: loss: 1.8670964241027832, accuracy: 0.375\n",
      "------------------------------------------------------------\n",
      "Train: loss : 0.5628745555877686, accuracy: 0.75\n",
      "Train: loss : 0.5958734154701233, accuracy: 0.75\n",
      "Train: loss : 0.8372088074684143, accuracy: 0.375\n",
      "Train: loss : 0.589931845664978, accuracy: 0.75\n",
      "Train: loss : 1.0038312673568726, accuracy: 0.5\n",
      "Test: loss: 0.6969332695007324, accuracy: 0.5\n",
      "Test: loss: 0.7538279891014099, accuracy: 0.125\n",
      "Test: loss: 1.49440598487854, accuracy: 0.375\n",
      "------------------------------------------------------------\n",
      "Train: loss : 0.6779683828353882, accuracy: 0.625\n",
      "Train: loss : 0.7081815004348755, accuracy: 0.375\n",
      "Train: loss : 0.8073264956474304, accuracy: 0.625\n",
      "Train: loss : 0.6136835217475891, accuracy: 0.75\n",
      "Train: loss : 0.5876795649528503, accuracy: 0.75\n",
      "Test: loss: 0.731578528881073, accuracy: 0.5\n",
      "Test: loss: 0.9404584169387817, accuracy: 0.125\n",
      "Test: loss: 1.78457510471344, accuracy: 0.375\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"-\" * 60)\n",
    "    for index, (batched_data, batched_labels) in enumerate(training_generator):\n",
    "        preds = model(batched_data)\n",
    "        loss = loss_func(preds, batched_labels)\n",
    "        accuracy = cal_acc(preds, batched_labels)\n",
    "        if( index % 50 == 0):\n",
    "            print(\"Train: loss : {0}, accuracy: {1}\".format(loss, accuracy))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    for index, (batched_data, batched_labels) in enumerate(testing_generator):\n",
    "        preds = model(batched_data)\n",
    "        loss = loss_func(preds, batched_labels)\n",
    "        accuracy = cal_acc(preds, batched_labels)\n",
    "        if(index % 40 == 0):\n",
    "            print(\"Test: loss: {0}, accuracy: {1}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-survivor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
