{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "another-acrobat",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Denylson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Denylson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Denylson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#Using for training and testing data creation\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import keras as keras\n",
    "\n",
    "#Used for model creation \n",
    "import torch as torch\n",
    "from torch import nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fourth-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name = '42B', dim = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "informed-conversion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2381\n"
     ]
    }
   ],
   "source": [
    "news1 = pd.read_csv('./djia_news/djia_news copy.csv')\n",
    "news2 = pd.read_csv('./nasdaq/nasdaq.csv')\n",
    "\n",
    "# combined_news = news1.append(news2)\n",
    "print(len(news1))\n",
    "news2 = news2.replace([0,1,2], ['decrease', 'increase', 'constant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "built-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Get_Modify():\n",
    "    \n",
    "    def __init__(self, dataframe, max_length):\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "        self.dataframe = self.dataframe[['Label', 'Headline']]\n",
    "        \n",
    "        self.label_dict = {'decrease' : 0, 'increase' : 1, 'constant' : 2}\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        self.dataframe = self.dataframe.apply(lambda x: x.astype(str).str.lower())\n",
    "        self.dataframe.Headline = self.dataframe.Headline.str.replace('[@:]','')\n",
    "        self.dataframe.Headline = self.dataframe.Headline.str.replace('totestravel', '')\n",
    "        self.dataframe['Headline_tokens'] = self.dataframe['Headline'].apply(lambda x: word_tokenize(x))\n",
    "        \n",
    "        all_words = []\n",
    "        for entry in self.dataframe['Headline_tokens']:\n",
    "            all_words += entry\n",
    "        frequency = Counter(all_words)\n",
    "        \n",
    "        self.vocab = torchtext.vocab.Vocab(counter = frequency, min_freq = 12, vectors = glove)\n",
    "        \n",
    "        self.dataframe = pd.DataFrame(self.dataframe)\n",
    "\n",
    "        #print(self.dataframe.head())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe['Headline'])\n",
    "    \n",
    "    def back_to_text(self, tokens):\n",
    "        text = ''\n",
    "        for token in tokens:\n",
    "            text += self.vocab.itos[token] + \" \"\n",
    "        return text\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label = self.label_dict[self.dataframe['Label'][index]]\n",
    "        label = torch.tensor(label)\n",
    "        \n",
    "        int_tokens = []\n",
    "        headline_tokens = self.dataframe['Headline_tokens'][index]\n",
    "        for token in headline_tokens:\n",
    "            int_tokens.append(self.vocab[token])\n",
    "        \n",
    "        if len(int_tokens) < self.max_length:\n",
    "            num_to_pad = self.max_length - len(int_tokens)\n",
    "            int_tokens += [0] * num_to_pad\n",
    "        else:\n",
    "            int_tokens = int_tokens[:self.max_length]\n",
    "        int_tokens = torch.tensor(int_tokens)\n",
    "        return(int_tokens, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "distributed-degree",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denylson\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3121"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Retreving data from csv files and merge both datasets\n",
    "news = Get_Modify(news2, 75)\n",
    "vocab = news.vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "laden-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determining training and test indices \n",
    "\n",
    "train_amount = int(0.70 * len(news))\n",
    "train_indices = list(range(0, train_amount))\n",
    "test_indices = list(range(train_amount, len(news)))\n",
    "\n",
    "#Creating subsets using the indices determined above\n",
    "training_data = Subset(news, train_indices)\n",
    "testing_data = Subset(news, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "amended-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the data to feed into model \n",
    "\n",
    "training_generator = DataLoader(training_data, batch_size = 8, shuffle = True)\n",
    "testing_generator = DataLoader(testing_data, batch_size = 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "stuck-raising",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 75]), torch.Size([8]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_data, batched_labels = next(iter(training_generator))\n",
    "batched_data.shape, batched_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "south-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creationg of the NLP model\n",
    "class News_NLP(nn.Module):\n",
    "    def __init__(self, num_words, emb_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_words = num_words\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        self.emb = nn.Embedding(self.num_words, self.emb_size)\n",
    "        self.emb.from_pretrained(vocab.vectors)\n",
    "        \n",
    "        self.lstm_1 = nn.LSTM(input_size = emb_size, hidden_size = 16, batch_first = True, num_layers = 2)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.gru = nn.GRU(input_size = 16, hidden_size = 32, batch_first = True, num_layers = 2)\n",
    "        self.sg_1 = nn.Sigmoid()\n",
    "        self.lstm_2 = nn.LSTM(input_size = 32, hidden_size = 64, batch_first = True, num_layers = 2, dropout = 0.2)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.rnn = nn.RNN(input_size = 64, hidden_size = 64, batch_first = True, num_layers = 2)\n",
    "        self.sg_2 = nn.Sigmoid()\n",
    "        self.lstm_3 = nn.LSTM(input_size = 64, hidden_size = 128, batch_first= True, num_layers = 1)\n",
    "        self.relu_3 = nn.ReLU()\n",
    "        self.linear = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, batch_data):\n",
    "        token_embs = self.emb(batch_data)\n",
    "        outputs, (h_n, c_n) = self.lstm_1(token_embs)\n",
    "        outputs, (h_n, c_n) = self.gru(outputs)\n",
    "        outputs, (h_n, c_n) = self.lstm_2(outputs)\n",
    "        outputs, (h_n, c_n) = self.rnn(outputs)\n",
    "        outputs, (h_n, c_n) = self.lstm_3(outputs)\n",
    "\n",
    "        \n",
    "        last_hidden_state = h_n\n",
    "        last_hidden_state = last_hidden_state.permute(1, 0, 2)\n",
    "        last_hidden_state = last_hidden_state.flatten(start_dim = 1)\n",
    "        last_hidden_state = self.relu_1(last_hidden_state)\n",
    "        last_hidden_state = self.sg_1(last_hidden_state)\n",
    "        last_hidden_state = self.relu_2(last_hidden_state)\n",
    "        last_hidden_state = self.sg_2(last_hidden_state)\n",
    "        last_hidden_state = self.relu_3(last_hidden_state)\n",
    "        \n",
    "        logits = self.linear(last_hidden_state)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "august-margin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News_NLP(\n",
       "  (emb): Embedding(3121, 300)\n",
       "  (lstm_1): LSTM(300, 16, num_layers=2, batch_first=True)\n",
       "  (relu_1): ReLU()\n",
       "  (gru): GRU(16, 32, num_layers=2, batch_first=True)\n",
       "  (sg_1): Sigmoid()\n",
       "  (lstm_2): LSTM(32, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (relu_2): ReLU()\n",
       "  (rnn): RNN(64, 64, num_layers=2, batch_first=True)\n",
       "  (sg_2): Sigmoid()\n",
       "  (lstm_3): LSTM(64, 128, batch_first=True)\n",
       "  (relu_3): ReLU()\n",
       "  (linear): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = News_NLP(num_words = len(vocab), emb_size = 300, num_classes = 3)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "atlantic-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "imported-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "toxic-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(preds, batched_labels):\n",
    "    predicted_labels = torch.softmax(preds, dim = 1).argmax(dim = 1)\n",
    "    num_correct = (predicted_labels == batched_labels).sum()\n",
    "    \n",
    "    acc = num_correct/len(batched_labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "charged-netscape",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Train: loss : 1.3747628927230835, accuracy: 0.0\n",
      "Train: loss : 1.1374882459640503, accuracy: 0.0\n",
      "Train: loss : 0.7337698936462402, accuracy: 0.5\n",
      "Train: loss : 0.6905331015586853, accuracy: 0.75\n",
      "Train: loss : 0.5956050157546997, accuracy: 0.75\n",
      "Train: loss : 0.8095293641090393, accuracy: 0.125\n",
      "Train: loss : 1.0229244232177734, accuracy: 0.625\n",
      "Train: loss : 0.7387509942054749, accuracy: 0.5\n",
      "Train: loss : 0.2825029790401459, accuracy: 1.0\n",
      "Train: loss : 1.028428077697754, accuracy: 0.625\n",
      "Train: loss : 0.6750720143318176, accuracy: 0.625\n",
      "Train: loss : 0.4446869492530823, accuracy: 1.0\n",
      "Test: loss: 2.2506632804870605, accuracy: 0.375\n",
      "Test: loss: 0.7930607199668884, accuracy: 0.25\n",
      "Test: loss: 0.7241982221603394, accuracy: 0.5\n",
      "Test: loss: 0.7586293816566467, accuracy: 0.375\n",
      "Test: loss: 0.6553356647491455, accuracy: 0.75\n",
      "Test: loss: 1.0283441543579102, accuracy: 0.75\n",
      "Test: loss: 0.6897668838500977, accuracy: 0.625\n",
      "Test: loss: 0.7241981625556946, accuracy: 0.5\n",
      "Test: loss: 1.0627752542495728, accuracy: 0.625\n",
      "Test: loss: 0.6897670030593872, accuracy: 0.625\n",
      "Test: loss: 1.5390777587890625, accuracy: 0.25\n",
      "Test: loss: 0.6209044456481934, accuracy: 0.875\n",
      "Test: loss: 1.4702152013778687, accuracy: 0.5\n",
      "------------------------------------------------------------\n",
      "Train: loss : 0.6553356647491455, accuracy: 0.75\n",
      "Train: loss : 0.7623153328895569, accuracy: 0.375\n",
      "Train: loss : 0.966395378112793, accuracy: 0.625\n",
      "Train: loss : 0.6549985408782959, accuracy: 1.0\n",
      "Train: loss : 1.1544133424758911, accuracy: 0.625\n",
      "Train: loss : 0.5637662410736084, accuracy: 0.75\n",
      "Train: loss : 0.9574673175811768, accuracy: 0.625\n",
      "Train: loss : 0.7020879983901978, accuracy: 0.5\n",
      "Train: loss : 0.7614012360572815, accuracy: 0.625\n",
      "Train: loss : 0.7512921094894409, accuracy: 0.5\n",
      "Train: loss : 0.5904099345207214, accuracy: 0.75\n",
      "Train: loss : 0.42530035972595215, accuracy: 0.875\n",
      "Test: loss: 3.1060309410095215, accuracy: 0.125\n",
      "Test: loss: 0.5932838916778564, accuracy: 0.75\n",
      "Test: loss: 0.741132915019989, accuracy: 0.5\n",
      "Test: loss: 0.6672083735466003, accuracy: 0.625\n",
      "Test: loss: 0.8889819383621216, accuracy: 0.25\n",
      "Test: loss: 1.498687505722046, accuracy: 0.125\n",
      "Test: loss: 0.8150574564933777, accuracy: 0.375\n",
      "Test: loss: 0.7411328554153442, accuracy: 0.5\n",
      "Test: loss: 1.4247630834579468, accuracy: 0.25\n",
      "Test: loss: 0.8150573968887329, accuracy: 0.375\n",
      "Test: loss: 1.812695026397705, accuracy: 0.5\n",
      "Test: loss: 0.9629063606262207, accuracy: 0.125\n",
      "Test: loss: 1.9605441093444824, accuracy: 0.25\n",
      "------------------------------------------------------------\n",
      "Train: loss : 1.2029895782470703, accuracy: 0.625\n",
      "Train: loss : 0.8181466460227966, accuracy: 0.25\n",
      "Train: loss : 0.8192843198776245, accuracy: 0.5\n",
      "Train: loss : 0.6975305676460266, accuracy: 0.625\n",
      "Train: loss : 0.733673632144928, accuracy: 0.625\n",
      "Train: loss : 1.2349700927734375, accuracy: 0.625\n",
      "Train: loss : 0.7493768334388733, accuracy: 0.5\n",
      "Train: loss : 0.6741647720336914, accuracy: 0.625\n",
      "Train: loss : 0.6742410063743591, accuracy: 0.625\n",
      "Train: loss : 1.0764424800872803, accuracy: 0.5\n",
      "Train: loss : 0.2460460364818573, accuracy: 1.0\n",
      "Train: loss : 0.7389136552810669, accuracy: 0.5\n",
      "Test: loss: 1.8904860019683838, accuracy: 0.375\n",
      "Test: loss: 0.7493276596069336, accuracy: 0.25\n",
      "Test: loss: 0.741952657699585, accuracy: 0.5\n",
      "Test: loss: 0.7456400990486145, accuracy: 0.375\n",
      "Test: loss: 0.7345777750015259, accuracy: 0.75\n",
      "Test: loss: 1.0207892656326294, accuracy: 0.75\n",
      "Test: loss: 0.7382652759552002, accuracy: 0.625\n",
      "Test: loss: 0.7419527769088745, accuracy: 0.5\n",
      "Test: loss: 1.0244767665863037, accuracy: 0.625\n",
      "Test: loss: 0.7382652759552002, accuracy: 0.625\n",
      "Test: loss: 1.3217504024505615, accuracy: 0.25\n",
      "Test: loss: 0.7308902740478516, accuracy: 0.875\n",
      "Test: loss: 1.3143755197525024, accuracy: 0.5\n",
      "------------------------------------------------------------\n",
      "Train: loss : 0.7382652759552002, accuracy: 0.625\n",
      "Train: loss : 0.8400964736938477, accuracy: 0.5\n",
      "Train: loss : 0.5399010181427002, accuracy: 0.875\n",
      "Train: loss : 1.1292604207992554, accuracy: 0.5\n",
      "Train: loss : 0.6249361038208008, accuracy: 0.75\n",
      "Train: loss : 0.9935107231140137, accuracy: 0.5\n",
      "Train: loss : 1.209513783454895, accuracy: 0.25\n",
      "Train: loss : 0.6450920104980469, accuracy: 0.75\n",
      "Train: loss : 0.9827525019645691, accuracy: 0.375\n",
      "Train: loss : 0.903414785861969, accuracy: 0.5\n",
      "Train: loss : 0.8571914434432983, accuracy: 0.625\n",
      "Train: loss : 0.6853268146514893, accuracy: 0.75\n",
      "Test: loss: 1.8958860635757446, accuracy: 0.375\n",
      "Test: loss: 0.921870231628418, accuracy: 0.25\n",
      "Test: loss: 0.7776209712028503, accuracy: 0.5\n",
      "Test: loss: 0.8497456312179565, accuracy: 0.375\n",
      "Test: loss: 0.6333717107772827, accuracy: 0.75\n",
      "Test: loss: 0.894906759262085, accuracy: 0.75\n",
      "Test: loss: 0.7054963111877441, accuracy: 0.625\n",
      "Test: loss: 0.7776209712028503, accuracy: 0.5\n",
      "Test: loss: 0.9670313596725464, accuracy: 0.625\n",
      "Test: loss: 0.7054963111877441, accuracy: 0.625\n",
      "Test: loss: 1.444940447807312, accuracy: 0.25\n",
      "Test: loss: 0.5612470507621765, accuracy: 0.875\n",
      "Test: loss: 1.3006912469863892, accuracy: 0.5\n",
      "------------------------------------------------------------\n",
      "Train: loss : 0.6333717107772827, accuracy: 0.75\n",
      "Train: loss : 0.6085538268089294, accuracy: 0.75\n",
      "Train: loss : 0.8229624629020691, accuracy: 0.625\n",
      "Train: loss : 0.7398952841758728, accuracy: 0.375\n",
      "Train: loss : 0.6316335797309875, accuracy: 0.75\n",
      "Train: loss : 0.35784608125686646, accuracy: 1.0\n",
      "Train: loss : 0.9066265225410461, accuracy: 0.25\n",
      "Train: loss : 0.7439382076263428, accuracy: 0.375\n",
      "Train: loss : 0.7340196967124939, accuracy: 0.625\n",
      "Train: loss : 1.1595028638839722, accuracy: 0.0\n",
      "Train: loss : 1.0803754329681396, accuracy: 0.625\n",
      "Train: loss : 0.6826879382133484, accuracy: 0.625\n",
      "Test: loss: 3.1592354774475098, accuracy: 0.375\n",
      "Test: loss: 0.9410891532897949, accuracy: 0.25\n",
      "Test: loss: 0.7603375911712646, accuracy: 0.5\n",
      "Test: loss: 0.8507134318351746, accuracy: 0.375\n",
      "Test: loss: 0.5795860290527344, accuracy: 0.75\n",
      "Test: loss: 1.1567163467407227, accuracy: 0.75\n",
      "Test: loss: 0.6699618101119995, accuracy: 0.625\n",
      "Test: loss: 0.7603374719619751, accuracy: 0.5\n",
      "Test: loss: 1.2470922470092773, accuracy: 0.625\n",
      "Test: loss: 0.6699617505073547, accuracy: 0.625\n",
      "Test: loss: 2.0953502655029297, accuracy: 0.25\n",
      "Test: loss: 0.48921018838882446, accuracy: 0.875\n",
      "Test: loss: 1.9145984649658203, accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"-\" * 60)\n",
    "    for index, (batched_data, batched_labels) in enumerate(training_generator):\n",
    "        preds = model(batched_data)\n",
    "        loss = loss_func(preds, batched_labels)\n",
    "        accuracy = cal_acc(preds, batched_labels)\n",
    "        if( index % 100 == 0):\n",
    "            print(\"Train: loss : {0}, accuracy: {1}\".format(loss, accuracy))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    for index, (batched_data, batched_labels) in enumerate(testing_generator):\n",
    "        preds = model(batched_data)\n",
    "        loss = loss_func(preds, batched_labels)\n",
    "        accuracy = cal_acc(preds, batched_labels)\n",
    "        if(index % 40 == 0):\n",
    "            print(\"Test: loss: {0}, accuracy: {1}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-survivor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
